{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('fr_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from spacy.lang.fr import French\n",
    "import spacy\n",
    "from line_profiler import LineProfiler\n",
    "from time import time\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.cli.download import download\n",
    "download('fr_core_news_md')\n",
    "import cProfile\n",
    "import io\n",
    "import pstats\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename= \"../../data/processed/DPEFs/dpef_paragraphs_sentences_long_format.csv\"\n",
    "df = pd.read_csv(output_filename,sep=\";\")\n",
    "if 'tokens' in df.columns:\n",
    "    df = df.drop('tokens', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile(fnc):\n",
    "    \n",
    "    def inner(*args, **kwargs):\n",
    "        pr = cProfile.Profile()\n",
    "        pr.enable()\n",
    "        rtval = fnc(*args, **kwargs)\n",
    "        pr.disable()\n",
    "        s = io.StringIO()\n",
    "        sortby = 'cumulative'\n",
    "        ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
    "        ps.print_stats()\n",
    "        print(s.getvalue())\n",
    "        return rtval\n",
    "    return inner     \n",
    "        \n",
    "#@profile  \n",
    "        \n",
    "\"\"\"\n",
    "Grammar module\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Grammar(object):\n",
    "    \"\"\"\n",
    "    Linguistic processing rules applied to sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # spaCy NLP\n",
    "        self.nlp = spacy.load(\"fr_core_news_md\")\n",
    "        \n",
    "    def create_match(self, matcher_name):\n",
    "        matcher = Matcher(self.nlp.vocab)\n",
    "        setattr(self, matcher_name, matcher)\n",
    "\n",
    "    def add_match(self, matcher_name, pattern_name, pattern):\n",
    "        getattr(self, matcher_name).add(pattern_name, None, pattern)\n",
    " \n",
    "    def parse(self, text):\n",
    "        \"\"\"\n",
    "        Parses text to NLP tokens.\n",
    "        Args:\n",
    "            text: input text\n",
    "        Returns:\n",
    "            tokens\n",
    "        \"\"\"\n",
    "        tokens = None\n",
    "        if text:\n",
    "            # Run text through linguistic rules\n",
    "            tokens = self.nlp(text)\n",
    "\n",
    "            # Apply custom rules to token list\n",
    "            #tokens = self.applyRules(tokens)\n",
    "        return tokens\n",
    "\n",
    "    def label(self, tokens):\n",
    "        \"\"\"\n",
    "        Linguistic rules processing logic. Identifies non-informative sentences and labels them accordingly.\n",
    "        Labels:\n",
    "            - QUESTION: Any text ending in a question mark\n",
    "            - FRAGMENT: Poorly structured sentences with limited information\n",
    "        Args:\n",
    "            tokens: parsed tokens\n",
    "        Returns:\n",
    "            label if detected else None\n",
    "        \"\"\"\n",
    "        label = None\n",
    "\n",
    "        if tokens:\n",
    "            # Label non-informative sentences\n",
    "            if self.isQuestion(tokens.text):\n",
    "                label = \"QUESTION\"\n",
    "            elif self.isFragment(tokens):\n",
    "                label = \"FRAGMENT\"\n",
    "        return label\n",
    "    \n",
    "    def isQuestion(self, text):\n",
    "        \"\"\"\n",
    "        Determines if the text is a question\n",
    "        Args:\n",
    "            text: input text\n",
    "        Returns:\n",
    "            true if text is a question, false otherwise\n",
    "        \"\"\"\n",
    "        # Questions have a ? mark at end\n",
    "        return text.strip().endswith(\"?\")\n",
    "\n",
    "    def isFragment(self, tokens):\n",
    "        \"\"\"\n",
    "        Run text against linguistic rules to determine if sentence is a fragment. Fragments are non descriptive.\n",
    "        Args:\n",
    "            tokens: nlp document tokens\n",
    "        Returns:\n",
    "            true if text is a sentence fragment, false otherwise\n",
    "        \"\"\"\n",
    "        # Nominal subject Nouns/Proper Nouns\n",
    "        nouns = any([t.pos_  in [\"NOUN\", \"PROPN\"]  and t.dep_ in [\"nsubj\", \"nsubjpass\"] for t in tokens])\n",
    "\n",
    "        # Actions (adverb, auxiliary, verb)\n",
    "        action = any([t.pos_ in [\"ADV\", \"AUX\", \"VERB\"] for t in tokens])\n",
    "\n",
    "        # Consider root words with a nominal subject as an action word\n",
    "        action = action or any([t.dep_ in [\"appos\", \"ROOT\"] and any([x.dep_ in [\"nsubj\", \"nsubjpass\"] for x in t.children]) for t in tokens])\n",
    "\n",
    "        # Non-punctuation tokens and multi-character words (don't count single letters which are often variables used in equations)\n",
    "        words = [t.text for t in tokens if t.pos_ not in [\"PUNCT\", \"SPACE\", \"SYM\"] and len(t.text) > 1]\n",
    "\n",
    "        # Valid sentences take the following form:\n",
    "        #  - At least one nominal subject noun/proper noun AND\n",
    "        #  - At least one action/verb AND\n",
    "        #  - At least 5 words\n",
    "        valid = nouns and action and len(words) >= 5\n",
    "\n",
    "        return not valid\n",
    "       \n",
    "    \n",
    "    def isPercent(self, tokens):\n",
    "        #We should be able to work this out with entities (any([t.label_ == \"PERCENT\" for t in doc.ents])\n",
    "\n",
    "        # Iterate over the tokens in the doc\n",
    "        for token in tokens[:-1]:\n",
    "            # Check if the token resembles a number\n",
    "            if token.like_num:\n",
    "                # Get the next token in the document\n",
    "                next_token = tokens[token.i+1]\n",
    "                # Check if the next token's text equals '%'\n",
    "                if next_token.text == \"%\":\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "            else:\n",
    "                return False\n",
    "        \n",
    "    def isDate(self, tokens):\n",
    "        #We should be able to work this out with entities (any([t.label_ == \"DATE\" for t in doc.ents])\n",
    "\n",
    "        # Iterate over the tokens in the doc\n",
    "        for token in tokens:\n",
    "            # Check if the token resembles a number\n",
    "            if token.like_num and token.is_digit:\n",
    "                if len(token)==4 and int(token.text)>2013:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "    \n",
    "    def commitLevel(self, tokens):\n",
    "        #Match for mentioning a commitment\n",
    "        self.create_match('comm_level_LOW')\n",
    "        pattern1 = [{\"LEMMA\": {\"IN\": [\"engager\", \"viser\", 'objectif', 'atteindre', 'prévoir' ]}}] \n",
    "        grammar.add_match('comm_level_LOW', 'pat_LOW', pattern1)\n",
    "\n",
    "        #Match for pattern [subj+commit_verb] such as 'Le Groupe s’engage...'\n",
    "        self.create_match('comm_level_MED')\n",
    "        pattern2 = [{\"POS\": {\"IN\": [\"PROPN\", \"NOUN\"]}},{\"POS\": \"PROPN\", \"OP\": \"?\"}, {\"POS\": \"AUX\", \"OP\": \"?\"},\n",
    "             {\"LEMMA\": {\"IN\": [\"engager\", \"viser\"]}}]\n",
    "        self.add_match('comm_level_MED', 'pat_MED', pattern2)\n",
    "        level = ''\n",
    "        if self.comm_level_LOW(tokens):\n",
    "            level = '1'+str(self.comm_level_LOW(tokens))\n",
    "            if self.comm_level_MED(tokens):\n",
    "                level = '2'+str(self.comm_level_MED(tokens))\n",
    "                \n",
    "                # Searching for label \"DATE\" or \"PERCENT\"\n",
    "                \"\"\"#Should work but problem with entities\n",
    "                if any([t.label_==\"DATE\" for t in tokens.ents]):\n",
    "                    level = '3'+str([t.label_ for t in tokens.ents])\"\"\"\n",
    "                if self.isPercent(tokens) or self.isDate(tokens):\n",
    "                    level = '3'\n",
    "        return level\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell can take a few minutes to run due to multiple function calls\n",
    "grammar = Grammar()\n",
    "df['tokens'] = df['sentence'].apply(grammar.parse)\n",
    "df['label']=df['tokens'].apply(grammar.label)\n",
    "df = df[df['label']!=('QUESTION' and 'FRAGMENT')]\n",
    "df['commit']=df['tokens'].apply(grammar.commitLevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename= \"../../data/processed/DPEFs/dpef_paragraphs_sentences_commit_level.csv\"\n",
    "df.to_csv(output_filename,sep=\";\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
